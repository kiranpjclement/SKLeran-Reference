{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### STRATIFIED SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test =train_test_split(X,y,test_size=0.2,random_state=0,stratify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIN MAX SCALING FOR TRAIN AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test) #test scaling done with the scalar used for train data to avoid data leakage( so not fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification_report  Recall /Precision/F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "print(classification_report(y_test,y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFICATION -BINARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common binary classification algo are KNN Cla, SVM,Random Forest ,MLP Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Advantages and disvantages of SVM"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The advantages of support vector machines are:\n",
    "\n",
    "Effective in high dimensional spaces.\n",
    "\n",
    "Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "\n",
    "Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "\n",
    "Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "\n",
    "SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "clf_svc=svm.SVC()\n",
    "clf_svc.fit(X_train,y_train)\n",
    "pred_clf_svc=clf_svc.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "print(classification_report(y_test,pred_clf_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "cls_rf = RandomForestClassifier(n_estimators=200,max_depth=2, random_state=0)#n_estimator is the only main hp we optimize in RF,\n",
    "#In practice, adjusting only one of these (e.g. max_depth) is enough to reduce overfitting\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "cls_rf.score(X_test,y_test)\n",
    "cls_rf.score(X_train,y_train)\n",
    "\n",
    "y_test_predict= cls_rf.predict(X_test)  # this is just to predict like a normal binary classification\n",
    "y_train_predict= cls_rf.predict(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict_prob=cls_rf.predict_proba(X_train) # for binary, it will give two rows , proba of 0 and 1, we need only one for score so [:,1]\n",
    "y_test_predict_prob=cls_rf.predict_proba(X_test)\n",
    "\n",
    "#after getting this predict proba , we can apply the threshold value to get 1 or 0\n",
    "#if thrshold .6 , <.6 is 0, and >0.6 is 1\n",
    "\n",
    "#how to find the best threshold ,this can be decided using the Roc_au_score grap/value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test,y_test_predict_prob[:,1])\n",
    "roc_auc_score(y_train,y_train_predict_prob[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_rf.predict(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hyperparameters to optimize in Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To avoid overfitting the training data, hyperparameters used in decision tree are\n",
    "\n",
    "max_depth - controls maximum depth (number of split points). Most common way to reduce tree complexity and overfitting.\n",
    "min_samples_split - the minimum number of samples a node must have before it can be split\n",
    "min_weight_fraction_leaf -same as min_samples_leaf but expressed as a fraction of the total number of weighted instances\n",
    "min_samples_leaf - The minimum number of samples required to be at a leaf node.\n",
    "max_leaf_nodes -maximum number of leaf nodes\n",
    "max_features-maximum number of features that are evaluated for splitting at each node.\n",
    "In practice, adjusting only one of these (e.g. max_depth) is enough to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlpc=MLPClassifier(hidden_layer_sizes =(11,11,11),max_iter=500)\n",
    "mlpc.fit(X_train,y_train)\n",
    "pred_mlpc =mlpc.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "print(classification_report(y_test,pred_mlpc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN -Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=3,weights='uniform',p=2,metric='minkowski',algorithm='auto') #creating an object of the model\n",
    "clf_knn.fit(X_train, y_train) #fitting the training data\n",
    "\n",
    "clf_knn.score(X_test,y_test)  #check the score of test data\n",
    "clf_knn.score(X_train,y_train)  # check the score of train data\n",
    "\n",
    "clf_knn.predict(X_predic)  # to predict the new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hyperparameter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.A distance metric\n",
    "  Typically Euclidean (Minkowski with p = 2)\n",
    "2.How many 'nearest' neighbors to look at?\n",
    "  e.g. five\n",
    "3.Optional weighting function on the neighbor points\n",
    "   Ignored\n",
    "4.How to aggregate the classes of neighbor points\n",
    "  Simple majority vote (Class with the most representatives among nearest neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Few topics to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "How sensitive is k-NN classification accuracy to the train/test split proportion ?\n",
    "How sensitive is k-NN classification accuracy to the choice of the 'k' parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radius Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "neigh = RadiusNeighborsClassifier(radius=1.0, weights='uniform', algorithm='auto',\n",
    "                                  leaf_size=30, p=2, metric='minkowski', outlier_label=None)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In cases where the data is not uniformly sampled, radius-based neighbors classification \n",
    "in RadiusNeighborsClassifier can be a better choice. The user specifies a fixed radius r, \n",
    "such that points in sparser neighborhoods use fewer nearest neighbors for the classification.\n",
    "For high-dimensional parameter spaces, this method becomes less effective due to the so-called “curse of dimensionality”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Important thing with Logistic and Linear is , the data points should be in Gaussian distributter for better performance\n",
    "###### so try to feature transform all/most of the features into Gaussian distrubution before applying it"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "L2 regularization is 'on' by default (like ridge regression)\n",
    "Parameter C controls amount of regularization (default 1.0)¶\n",
    "As with regularized linear regression, it can be important to normalize all features so that they are on the same scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "clf_log = LogisticRegression(penalty='l2',dual=False,tol=0.0001,C=1.0,fit_intercept=True,\n",
    "                             intercept_scaling=1,class_weight=None,random_state=None,\n",
    "                             solver='lbfgs', max_iter=100,multi_class='auto', verbose=0, \n",
    "                             warm_start=False,n_jobs=None,l1_ratio=None)\n",
    "clf_log.fit(X_train, y_train)\n",
    "\n",
    "clf_log.score(X_test,y_test)\n",
    "clf_log.score(X_train,y_train)\n",
    "print(classification_report(y_test,y_test_predict).\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Main Hyper parameters for logistic regression\n",
    "\n",
    "penalty: 'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
    "\n",
    "C : small values of C denoltes strong regularization   default =1    #is called inverse of regularization\n",
    "\n",
    "Solver : 'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga', default='lbfgs'\n",
    "For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.\n",
    "For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ \n",
    "is limited to one-versus-rest schemes.\n",
    "\n",
    "random_stateint, RandomState instance, default=None\n",
    "Used when solver == ‘sag’, ‘saga’ or ‘liblinear’ to shuffle the data\n",
    "\n",
    "max_iterint : default=100\n",
    "Maximum number of iterations taken for the solvers to converge."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For a multi_class problem, if multi_class is set to be “multinomial” the softmax function is used to find the\n",
    "predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class\n",
    "assuming it to be positive using the logistic function. and normalize these values across all the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_log = LogisticRegression(penalty='l2',dual=False,tol=0.0001,C=1.0,fit_intercept=True,\n",
    "                             intercept_scaling=1,class_weight=None,random_state=None,\n",
    "                             solver='lbfgs', max_iter=100,multi_class='multinomial', verbose=0, \n",
    "                             warm_start=False,n_jobs=None,l1_ratio=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECISION TREE CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, \n",
    "                              min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                              max_features=None, random_state=None, max_leaf_nodes=None, \n",
    "                              min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)\n",
    "\n",
    "clf_dt.score(X_test,y_test)\n",
    "clf_dt.score(X_train,y_train)\n",
    "\n",
    "cls_dt.predict_proba(X_predict)\n",
    "cls_dt.predict(X_predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Feature Importance in Forest of Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Based on Mean Decrease in Impurity6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = forest.feature_importances_\n",
    "std = np.std([\n",
    "    tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "\n",
    "#Plotting ax= features\n",
    "\n",
    "import pandas as pd\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Based on Feature permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(\n",
    "    forest, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "\n",
    "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "\n",
    "#Ploting ax=features\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation on full model\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### directly ploting the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf=random forest object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rf.feature_importances)\n",
    "plt.xticks(np.arange(X.shape[1]),X.columns.tolist().rotation=90):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf_ab = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "clf_ab.fit(X_train, y_train)\n",
    "\n",
    "clf.score(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xg boost has an algo to fill the NAn value automatically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN -Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://sklearn.org/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor\n",
    "    \n",
    "https://sklearn.org/modules/neighbors.html#regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "reg_knn = KNeighborsRegressor(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski')\n",
    "reg_knn.fit(X_train, y_train)\n",
    "\n",
    "reg_knn.score(X_test, y_test)\n",
    "reg_knn.score(X_train,y_train)\n",
    "\n",
    "reg_knn.predict(y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n",
    "‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a\n",
    "    query point will have a greater influence than neighbors which are further away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Algorithm used to compute the nearest neighbors:\n",
    "\n",
    "‘ball_tree’ will use BallTree\n",
    "‘kd_tree’ will use KDTree\n",
    "‘brute’ will use a brute-force search.\n",
    "‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Important thing with Logistic and Linear is , the data points should be in Gaussian distributter for better performance\n",
    "###### so try to feature transform all/most of the features into Gaussian distrubution before applying it"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A linear model is a sum of weighted variables that predicts a target output value given an input data instance.\n",
    "eg:Linear Regression ,Logistic Regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When to use ridge vs lasso regression:\n",
    "Many small/medium sized effects: use ridge.\n",
    "Only a few variables with medium/large effect: use lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classsical Linear Regression4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg_lin = LinearRegression().fit(X, y)\n",
    "\n",
    "reg_lin.score(X_test,y_test)\n",
    "reg_lin.score(X_train,y_train)\n",
    "\n",
    "reg_lin.predict(X_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_   #to get the coefficients or wts of each feature\n",
    "\n",
    "reg.intercept_  #gives the intercept of the line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR -SVM for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "print('rbf',svm_rbf.score(X_train, y_train),svm_rbf.score(X_test, y_test))\n",
    "\n",
    "svr_lin = SVR(kernel='linear', C=100, gamma='auto')\n",
    "svr_lin.fit(X_train, y_train)\n",
    "print('lin',svm_lin.score(X_train, y_train),svm_lin.score(X_test, y_test))\n",
    "\n",
    "svr_poly = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1,coef0=1)\n",
    "svr_poly.fit(X_train, y_train)\n",
    "print('poly',svm_poly.score(X_train, y_train),svm_poly.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hyper parameters SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "C - = +ve infi Higher value of C means less regularization and will not misclassify in traning set and leads to overfitting.\n",
    "    =0         more regularization and misclassify more data during training can cause underfitting\n",
    "we should have a balance in C, \n",
    "\n",
    "\n",
    "\n",
    "kernel: Type of kernel function to be used\n",
    "Default = 'rbf' for radial basis function\n",
    "Kernel =  'rbf', 'linear', 'poly'\n",
    "\n",
    "KERNEL PARAMETERS\n",
    "gamma (γγ): RBF kernel width\n",
    "C: regularization parameter\n",
    "Typically C and gamma are tuned at the same time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RIDGE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ridge regression learns w, b using the same least-squares criterion but adds a penalty for large variations in w parameters.\n",
    "The addition of a parameter penalty is called regularization. Regularization prevents overfitting by restricting the model, typically to reduce its complexity.\n",
    "Ridge regression uses L2 regularization: minimize sum of squares of w entries\n",
    "The influence of the regularization term is controlled by the α parameter.\n",
    "Higher alpha means more regularization and simpler models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "L1 penalty: Minimize the sum of the absolute values of the coefficients\n",
    "This has the effect of setting parameter weights in w to zero for the least influential variables. This is called a sparse solution: a kind of feature selection¶\n",
    "The parameter αα controls amount of L1 regularization (default = 1.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When to use ridge vs lasso regression:\n",
    "Many small/medium sized effects: use ridge.\n",
    "Only a few variables with medium/large effect: use lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POLYNOMIAL REGRESSION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To capture interactions between the original features by adding them as features to the linear model.\n",
    "To make a classification problem easier\n",
    "\n",
    "Thus, polynomial feature expansion is often combined with a regularized learning method like ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### difference between linear svc and svc ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf_svm = LinearSVC(random_state=0, max_iter=1000, dual=False,C=10,kernel = 'rbf', gamma =0.1)\n",
    "\n",
    "\n",
    "#svm_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
    "#svr_lin = SVR(kernel='linear', C=100, gamma='auto')\n",
    "#svr_poly = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1,coef0=1)\n",
    "    \n",
    "clf_svm.fit(X_train, y_train)\n",
    "\n",
    "clf_svm.score(X_train, y_train)\n",
    "clf_svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In simple regression, we try to minimize the error rate. While in SVR we try to fit the error within a certain threshold."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
